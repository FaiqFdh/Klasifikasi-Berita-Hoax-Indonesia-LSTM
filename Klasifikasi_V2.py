# -*- coding: utf-8 -*-
"""Bab 4 V2 of Klasifikasi_Berita_Hoax_Tanpa_Stemming_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17d-lesbObuJWgqvO8iEwI9tph61pU1mZ
"""

# import zipfile
#
# zip_ref = zipfile.ZipFile('/content/indonesian-fact-and-hoax-political-news.zip', 'r')
# zip_ref.extractall('/content')
# zip_ref.close()

"""# Reading Data"""
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from keras.src.optimizers import Adam
import nltk
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from proses import preprocessing, generate_and_display_wordcloud, split_data, plot_graphs, \
    evaluate_and_visualize, predict_sentences

fact_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_cnn_10k_cleaned.xlsx')
#fact_df['Hoax'] = 0

hoax_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_turnbackhoax_10_cleaned.xlsx')
#hoax_df['Hoax'] = 1


print(fact_df)

print(fact_df.dtypes)

print(hoax_df.dtypes)

"""## Cek Kolom"""

print(fact_df.columns)

fact_df = fact_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','text_new'])

print(fact_df)

print(hoax_df)

print(hoax_df.columns)

hoax_df = hoax_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','politik','Narasi','Clean Narasi'])

print(hoax_df)

print(hoax_df[hoax_df['Title'].isna()])

#Menghilangkan kata [SALAH] di judul Hoax
hoax_df = hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)]
print(hoax_df.head())

print("\nSetelah Remove [SALAH]")
# Menghapus kata '[SALAH]' dari kolom 'Title'
hoax_df['Title'] = hoax_df['Title'].str.replace(r'\[SALAH\]', '',regex=True)
print(hoax_df.head())


#Menghapus Tanda Kutip dari Judul Berita Hoax
# Menghilangkan tanda kutip di awal dan akhir teks dalam kolom 'Title'
print('\nSetelah Remove "" Tanda Kutip')
#hoax_df['Title'] = hoax_df['Title'].str.strip('“”')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('“”')
#hoax_df['Title'] = hoax_df['Title'].str.strip('"')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('""')

print(hoax_df)

print(hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)])

"""## Kombinasi Fact and Hoax"""

news_df = pd.concat([fact_df.iloc[:], hoax_df.iloc[:]], axis=0, ignore_index=True)
#news_df = pd.concat([fact_df.iloc[:], fact_df1.iloc[:],hoax_df.iloc[:],], axis=0, ignore_index=True)

print(news_df)

"""## Representasi Jumlah Data"""

# Menghitung jumlah angka 0 dan 1 dalam kolom "hoax"
hoax_count = news_df['hoax'].value_counts()

# Warna untuk masing-masing nilai
colors = ['blue', 'orange']

# Membuat diagram batang dengan warna yang berbeda untuk masing-masing nilai
plt.bar(hoax_count.index, hoax_count.values, color=colors)
#plt.xlabel('Hoax')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Tidak Hoax', 'Hoax'])
plt.title('Distribusi Data Berita per Kategori')
plt.show()

# download corpus punkt
nltk.download('punkt')

text = "Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan"
print(word_tokenize(text))

print(news_df.dtypes)

news_df.count()

"""# Preprocessing

## FUNGSI PREPROCESSING FIX
"""

# Contoh penggunaan:
news_df = preprocessing(news_df)

print(news_df)

"""## WordCloud Real , RUN SETELAH REMOVE STOPWORD"""

# Contoh penggunaan fungsi untuk non-hoax
generate_and_display_wordcloud(news_df, label_value=0)

# Contoh penggunaan fungsi untuk hoax
generate_and_display_wordcloud(news_df, label_value=1)

"""## Defining useful global variables

Next, you will define some global variables that will be used in the unit tests after your solutions. **Please do not use these in the function body of the graded functions.**

- `NUM_WORDS`: The maximum number of words to keep, based on word frequency. Defaults to 1000.


- `EMBEDDING_DIM`: Dimension of the dense embedding, will be used in the embedding layer of the model. Defaults to 16.


- `MAXLEN`: Maximum length of all sequences. Defaults to 120.


- `PADDING`: Padding strategy (pad either before or after each sequence.). Defaults to 'post'.


- `OOV_TOKEN`: Token to replace out-of-vocabulary words during text_to_sequence calls. Defaults to "\<OOV>".

    
- `TRAINING_SPLIT`: Proportion of data used for training. Defaults to 0.8

**For now leave them unchanged but after submitting your assignment for grading you are encouraged to come back here and play with these parameters to see the impact they have in the classification process**
"""

# grader-required-cell

#NUM_WORDS = 12000
EMBEDDING_DIM = 50
MAXLEN = 30
PADDING = 'post'
OOV_TOKEN = "<OOV>"
TRAINING_SPLIT = .8

# Splitting the data into 80% training and 20% testing
#x_train, x_test, y_train, y_test = train_test_split(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)
# Gantilah news_df['Title'] dan news_df['hoax'] sesuai dengan nama kolom yang sesuai dalam DataFrame Anda.
x_train, x_test, y_train, y_test = split_data(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)

print("Jumlah data dalam set pelatihan:", x_train.shape[0])
print("Jumlah data dalam set pengujian:", x_test.shape[0])

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(x_train)

print(x_test)

print(y_train)

print(y_test)

"""## 4.Tokenizing"""

#Tokenizing Text -> Repsesenting each word by a number
#tokenizer = Tokenizer(num_words = NUM_WORDS,oov_token = OOV_TOKEN)
max_features = 15000
maxlen = 30

tokenizer = Tokenizer(num_words=max_features,oov_token = OOV_TOKEN)
tokenizer.fit_on_texts(x_train)
tokenized_train = tokenizer.texts_to_sequences(x_train)

#padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)
x_train = pad_sequences(tokenized_train,padding=PADDING, maxlen=maxlen)

tokenized_test = tokenizer.texts_to_sequences(x_test)
X_test = pad_sequences(tokenized_test,padding=PADDING ,maxlen=maxlen)

print(x_train)

print(x_train[24])

word_index = tokenizer.word_index

#print(word_index)

print(len(word_index))

max_features = 15000
batch_size = 64
epochs = 5
# 10 terlalu banyak , overfitting
embed_size = 100

# Function to build the model
"""## MODEL

### Model 1

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.01
"""

def build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate):
    max_features =  15000# Isi dengan nilai sesuai kebutuhan
    maxlen =  30 # Isi dengan nilai sesuai kebutuhan
    # x_train, y_train, X_test, y_test harus sudah didefinisikan sebelumnya

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=max_features, output_dim=embed_size, input_length=maxlen),
        tf.keras.layers.LSTM(hidden_layer_size),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])

    model.summary()

    history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), epochs=epochs)

    return history,model

# Contoh pemanggilan fungsi:
epochs = 5
embed_size = 50
hidden_layer_size = 64
batch_size = 64
learning_rate = 0.01

print("Model 1")
# Contoh pemanggilan untuk dua model dengan nama yang berbeda
history,model = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Usage
evaluate_and_visualize(model, x_train, y_train, X_test, y_test)

# Example usage
input_sentences = ["anies meninggal dunia", "jokowi minta maaf kepada pki","gede pasek doakan ahy menjadi capres","CAK NUN SEBUT JOKOWI SEPERTI FIR’AUN KARENA DISURUH",
                   "Beban Utang Prabowo Subianto 7,6 Triliun"]

predictions = predict_sentences(input_sentences, model, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 2

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.001

"""

epochs = 5
embed_size = 100
hidden_layer_size = 64
batch_size = 64
learning_rate = 1e-2
print("Model 2")
history2,model2 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history2, "accuracy")
plot_graphs(history2, "loss")

# Usage
evaluate_and_visualize(model2, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model2, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 3

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.0001

"""

epochs = 5
embed_size = 100
hidden_layer_size = 64
batch_size = 64
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 3")
history3,model3 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history3, "accuracy")
plot_graphs(history3, "loss")

# Usage
evaluate_and_visualize(model3, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model3, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 4

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 32
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.01
"""

epochs = 5
embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-1

print("Model 4")
history4,model4 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history4, "accuracy")
plot_graphs(history4, "loss")

# Usage
evaluate_and_visualize(model4, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model4, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 5

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 32
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.001
"""

epochs = 5
embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-2

print("Model 5")
history5,model5 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history5, "accuracy")
plot_graphs(history5, "loss")

# Usage
evaluate_and_visualize(model5, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model5, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 6

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 32
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.0001
"""

epochs = 5
embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 6")
history6,model6 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history6, "accuracy")
plot_graphs(history6, "loss")

# Usage
evaluate_and_visualize(model6, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model6, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

