# -*- coding: utf-8 -*-
"""Klasifikasi_Berita_Word2Vec_50Dimensi_FIX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bdVYw1zsJBMuVc8hgGYDY0Uvlfml9HOw

Download Dataset
"""

"""# Reading Data"""
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from keras.src.optimizers import Adam
import nltk
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from proses import preprocessing, generate_and_display_wordcloud, split_data, plot_graphs, \
    evaluate_and_visualize, predict_sentences

fact_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_cnn_10k_cleaned.xlsx')
#fact_df['Hoax'] = 0

hoax_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_turnbackhoax_10_cleaned.xlsx')
#hoax_df['Hoax'] = 1


print(fact_df)

print(fact_df.dtypes)

print(hoax_df.dtypes)

"""## Cek Kolom"""

print(fact_df.columns)

fact_df = fact_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','text_new'])

print(fact_df)

print(hoax_df)

print(hoax_df.columns)

hoax_df = hoax_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','politik','Narasi','Clean Narasi'])

print(hoax_df)

print(hoax_df[hoax_df['Title'].isna()])

#Menghilangkan kata [SALAH] di judul Hoax
hoax_df = hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)]
print(hoax_df.head())

print("\nSetelah Remove [SALAH]")
# Menghapus kata '[SALAH]' dari kolom 'Title'
hoax_df['Title'] = hoax_df['Title'].str.replace(r'\[SALAH\]', '',regex=True)
print(hoax_df.head())


#Menghapus Tanda Kutip dari Judul Berita Hoax
# Menghilangkan tanda kutip di awal dan akhir teks dalam kolom 'Title'
print('\nSetelah Remove "" Tanda Kutip')
#hoax_df['Title'] = hoax_df['Title'].str.strip('“”')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('“”')
#hoax_df['Title'] = hoax_df['Title'].str.strip('"')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('""')

hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('“”')
#hoax_df['Title'] = hoax_df['Title'].str.strip('"')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('‘’')
# “” ″
# ‘’
print(hoax_df)

print(hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)])

"""## Kombinasi Fact and Hoax"""

news_df = pd.concat([fact_df.iloc[:], hoax_df.iloc[:]], axis=0, ignore_index=True)
#news_df = pd.concat([fact_df.iloc[:], fact_df1.iloc[:],hoax_df.iloc[:],], axis=0, ignore_index=True)

print(news_df)

"""## Representasi Jumlah Data"""

# Menghitung jumlah angka 0 dan 1 dalam kolom "hoax"
hoax_count = news_df['hoax'].value_counts()

# Warna untuk masing-masing nilai
colors = ['blue', 'orange']

# Membuat diagram batang dengan warna yang berbeda untuk masing-masing nilai
plt.bar(hoax_count.index, hoax_count.values, color=colors)
#plt.xlabel('Hoax')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Tidak Hoax', 'Hoax'])
plt.title('Distribusi Data Berita per Kategori')
plt.show()

# download corpus punkt
nltk.download('punkt')

text = "Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan"
print(word_tokenize(text))

print(news_df.dtypes)

news_df.count()

"""# Preprocessing

## FUNGSI PREPROCESSING FIX
"""

# Contoh penggunaan:
news_df = preprocessing(news_df)

print(news_df)

# Mengecek keberadaan karakter "”", "″", "‘", "’", dan emotikon di dalam kolom "title"
for character in ["”", "″", "‘", "’"]:
    contains_character = news_df['Title'].str.contains(character)
    print(f"Contains '{character}':\n{news_df[contains_character]}")

# Mengecek keberadaan emotikon di dalam kolom "title"
emoticon_pattern = "["
emoticon_pattern += u"\U0001F600-\U0001F64F"  # Emotikon umum
emoticon_pattern += u"\U0001F300-\U0001F5FF"  # Simbol & peta
emoticon_pattern += u"\U0001F680-\U0001F6FF"  # Transportasi & simbol umum
emoticon_pattern += u"\U0001F700-\U0001F77F"  # Alat & simbol teknis
emoticon_pattern += u"\U0001F780-\U0001F7FF"  # Alat & seni
emoticon_pattern += u"\U0001F800-\U0001F8FF"  # Variasi warna
emoticon_pattern += u"\U0001F900-\U0001F9FF"  # Emoji & simbol tambahan
emoticon_pattern += u"\U0001FA00-\U0001FA6F"  # Emoji & simbol tambahan
emoticon_pattern += u"\U0001FA70-\U0001FAFF"  # Emoji & simbol tambahan
emoticon_pattern += u"\U00002702-\U000027B0"  # Emoji & simbol tambahan
emoticon_pattern += u"\U000024C2-\U0001F251"
emoticon_pattern += "]+"

contains_emoticon = news_df['Title'].str.contains(emoticon_pattern, flags=re.UNICODE)
print(f"Contains emoticon:\n{news_df[contains_emoticon]}")

"""## WordCloud Real , RUN SETELAH REMOVE STOPWORD"""

# Contoh penggunaan fungsi untuk non-hoax
generate_and_display_wordcloud(news_df, label_value=0)

# Contoh penggunaan fungsi untuk hoax
generate_and_display_wordcloud(news_df, label_value=1)

"""## Split Train Test"""

# grader-required-cell

EMBEDDING_DIM = 50
PADDING = 'post'
OOV_TOKEN = "<OOV>"
TRAINING_SPLIT = .8

# Splitting the data into 80% training and 20% testing
#x_train, x_test, y_train, y_test = train_test_split(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)
x_train, x_test, y_train, y_test = split_data(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)

print("Jumlah data dalam set pelatihan:", x_train.shape[0])
print("Jumlah data dalam set pengujian:", x_test.shape[0])

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(x_train)

print(x_test)

print(y_train)

print(y_test)

# Membuat DataFrame
df_result = pd.DataFrame({
    'Text': x_test,  # Gantilah dengan nama kolom sesuai dengan data yang ada pada X_test
    'True_Label': y_test,  # Gantilah dengan nama kolom sesuai dengan data yang ada pada y_test
})

# Reset index
#df_result = df_result.reset_index(drop=True)

# Menyimpan DataFrame yang sudah diperbarui
df_result.to_csv('df_data_test.csv', index=False)  # Gantilah dengan nama file dan path yang sesuai

print(df_result)

"""## Tokenizing dan Padding"""

"""## 4.Tokenizing"""

#Tokenizing Text -> Repsesenting each word by a number
#tokenizer = Tokenizer(num_words = NUM_WORDS,oov_token = OOV_TOKEN)
max_features = 15000
maxlen = 30

tokenizer = Tokenizer(num_words=max_features,oov_token = OOV_TOKEN)
tokenizer.fit_on_texts(x_train)
tokenized_train = tokenizer.texts_to_sequences(x_train)

#padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)
x_train = pad_sequences(tokenized_train,padding=PADDING, maxlen=maxlen)

tokenized_test = tokenizer.texts_to_sequences(x_test)
X_test = pad_sequences(tokenized_test,padding=PADDING ,maxlen=maxlen)

print(x_train)

print(x_train[24])

word_index = tokenizer.word_index

#print(word_index)

print(len(word_index))

"""## Model Word2Vec 50 Dimension"""

import numpy as np
from nltk.tokenize import word_tokenize

embeddings_dictionary = {}
with open('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia Word2Vec FIX\\Indo Word2Vec\\word2vec.txt') as fp:
    for line in fp.readlines():
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions

vocab_length = 15000 # modify
embedding_dim = 50

embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in tokenizer.word_index.items(): # change depending on your data
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

vocab_length = len(embeddings_dictionary)
print("Vocabulary Length:", vocab_length)

print(embedding_matrix)

print(embedding_vector)

# Get the dimensions from the loaded embeddings
print(embedding_vector.shape[0])

print(embedding_matrix.shape[0])

"""### Cek vektor kata dan yang berdekatan"""

from sklearn.metrics.pairwise import cosine_similarity

# Mencari vektor kata "saya"
vector_saya = embeddings_dictionary.get("saya")

# Mencari kata-kata berdekatan berdasarkan cosine similarity
similar_words = {}
for word, vector in embeddings_dictionary.items():
    similarity = cosine_similarity([vector_saya], [vector])[0][0]
    similar_words[word] = similarity

# Mengurutkan kata-kata berdasarkan similarity (dalam urutan menurun)
similar_words = sorted(similar_words.items(), key=lambda x: x[1], reverse=True)

# Menampilkan hasil
print("Kata-kata berdekatan dengan 'saya':")
for word, similarity in similar_words[:10]:  # Ambil 10 kata teratas
    print(f"{word}: {similarity}")

"""## Build Model with Word2Vec

### Model 1
"""

max_features = 15000
batch_size = 64
epochs = 5
# 10 terlalu banyak , overfitting
embed_size = 50

# Function to build the model
"""## MODEL

### Model 1

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.01
"""

def build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate):
    max_features =  15000# Isi dengan nilai sesuai kebutuhan
    maxlen =  30 # Isi dengan nilai sesuai kebutuhan
    # x_train, y_train, X_test, y_test harus sudah didefinisikan sebelumnya

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(
        input_dim=max_features,
        output_dim=embedding_dim,  # Adjusted to match the pre-trained embedding dimension
        weights=[embedding_matrix],
        input_length=maxlen,
        trainable=False  # Optional: Set to True if you want to fine-tune the embeddings
    ),
        #tf.keras.layers.Embedding(input_dim=max_features, output_dim=embed_size, input_length=maxlen),
        tf.keras.layers.LSTM(hidden_layer_size),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])

    model.summary()

    history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), epochs=epochs)

    return history,model

# Contoh pemanggilan fungsi:
#epochs = 5
#embed_size = 50
hidden_layer_size = 64
#batch_size = 64
learning_rate = 0.01

print("Model 1")
# Contoh pemanggilan untuk dua model dengan nama yang berbeda
history,model = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model.save("model_lstm1.h5")
print("Model telah disimpan sebagai 'model_lstm1.h5'.")

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Usage
evaluate_and_visualize(model, x_train, y_train, X_test, y_test)

# Example usage
input_sentences = ["anies meninggal dunia", "jokowi minta maaf kepada pki","gede pasek doakan ahy menjadi capres","CAK NUN SEBUT JOKOWI SEPERTI FIR’AUN KARENA DISURUH",
                   "Beban Utang Prabowo Subianto 7,6 Triliun"]

predictions = predict_sentences(input_sentences, model, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 2"""

"""### Model 2

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.001

"""

#epochs = 5
#embed_size = 50
hidden_layer_size = 64
batch_size = 64
learning_rate = 1e-2

print("Model 2")
history2,model2 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model2.save("model_lstm2.h5")
print("Model telah disimpan sebagai 'model_lstm2.h5'.")

plot_graphs(history2, "accuracy")
plot_graphs(history2, "loss")

# Usage
evaluate_and_visualize(model2, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model2, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 3"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 64
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 3")
history3,model3 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model3.save("model_lstm3.h5")
print("Model telah disimpan sebagai 'model_lstm3.h5'.")

plot_graphs(history3, "accuracy")
plot_graphs(history3, "loss")

# Usage
evaluate_and_visualize(model3, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model3, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 4"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-1

print("Model 4")
history4,model4 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model4.save("model_lstm4.h5")
print("Model telah disimpan sebagai 'model_lstm4.h5'.")

plot_graphs(history4, "accuracy")
plot_graphs(history4, "loss")

# Usage
evaluate_and_visualize(model4, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model4, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 5"""

epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-2

print("Model 5")
history5,model5 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model5.save("model_lstm5.h5")
print("Model telah disimpan sebagai 'model_lstm5.h5'.")

plot_graphs(history5, "accuracy")
plot_graphs(history5, "loss")

# Usage
evaluate_and_visualize(model5, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model5, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 6"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 6")
history6,model6 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

# Menyimpan model di luar fungsi
model6.save("model_lstm6.h5")
print("Model telah disimpan sebagai 'model_lstm6.h5'.")

plot_graphs(history6, "accuracy")
plot_graphs(history6, "loss")

# Usage
evaluate_and_visualize(model6, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model6, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

## TEST KODE LABEL PREDICT == REAL
# Misalkan Anda sudah mempunyai DataFrame df_result
# df_result = ...

# Misalnya, sentences adalah kolom 'text' pada DataFrame
sentences = df_result['Text']

max_len =30

# Memanggil fungsi predict_sentences untuk memperoleh prediksi biner
binary_predictions = predict_sentences(sentences, model3, tokenizer, max_len)

# Menambahkan kolom 'predicted_label' ke DataFrame
df_result['predicted_label'] = binary_predictions

# Menambahkan kolom 'is_correct' yang berisi label true jika 'predicted_label' sama dengan 'true_label'
df_result['is_correct'] = df_result['predicted_label'] == df_result['True_Label']

print(df_result)

# Menghitung jumlah label true dan false dari kolom 'is_correct'
label_counts = df_result['is_correct'].value_counts()

# Menampilkan hasil
print("Jumlah label True:", label_counts[True])
print("Jumlah label False:", label_counts[False])

# Menyimpan DataFrame yang sudah diperbarui
df_result.to_csv('df_result_model3.csv', index=False)  # Gantilah dengan nama file dan path yang sesuai

from keras.models import load_model

model_load = load_model('model_lstm3.h5')
model.summary()

# Example usage
input_sentences = ["foto selebaran waspada penculikan anak anak berumur polda metro jaya",
                   "dpr aturan disiplin kerja pns terarah",
                   "projo patuh konstitusi sikapi isu penundaan pemilu",
                   "peserta pemilu partai ummat nomor urut",
                   "foto rumah sakit tega meloloskan bapa membawa pulang mayat anaknya sepeda motor"]

predictions = predict_sentences(input_sentences, model_load, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

import tkinter as tk
from tkinter import messagebox

# Function to handle button click event
def predict():
    input_text = entry.get()
    if not input_text:
        messagebox.showinfo("Info", "Please enter a sentence.")
        return

    # Call predict_sentences with the input text
    predictions = predict_sentences([input_text], model_load, tokenizer, maxlen)

    print("prediksi sebelum [0][0]", predictions)

    #result = "Hoax" if predictions[0][0] == 1 else "Real"
    result = "Hoax" if predictions == 1 else "Real"
    print("Prediksi di tkinter " , predictions)
    messagebox.showinfo("Prediction Result", f"The sentence is likely: {result}")

# Create the main application window
app = tk.Tk()
app.title("Hoax Detector")

# Create GUI components
label = tk.Label(app, text="Judul Kalimat:")
label.pack()

entry = tk.Entry(app, width=100)
entry.pack()

predict_button = tk.Button(app, text="Prediksi", command=predict)
predict_button.pack()

# Start the Tkinter event loop
app.mainloop()
