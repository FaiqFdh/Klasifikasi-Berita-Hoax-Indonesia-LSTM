# -*- coding: utf-8 -*-
"""Klasifikasi_Berita_Word2Vec_50Dimensi_FIX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bdVYw1zsJBMuVc8hgGYDY0Uvlfml9HOw

Download Dataset
"""

"""# Reading Data"""
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from keras.src.optimizers import Adam
import nltk
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from proses import preprocessing, generate_and_display_wordcloud, split_data, plot_graphs, \
    evaluate_and_visualize, predict_sentences

fact_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_cnn_10k_cleaned.xlsx')
#fact_df['Hoax'] = 0

hoax_df = pd.read_excel('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia NLP\\Dataset\\dataset_turnbackhoax_10_cleaned.xlsx')
#hoax_df['Hoax'] = 1


print(fact_df)

print(fact_df.dtypes)

print(hoax_df.dtypes)

"""## Cek Kolom"""

print(fact_df.columns)

fact_df = fact_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','text_new'])

print(fact_df)

print(hoax_df)

print(hoax_df.columns)

hoax_df = hoax_df.drop(columns=['Unnamed: 0','Timestamp','FullText','Tags','Author','Url','politik','Narasi','Clean Narasi'])

print(hoax_df)

print(hoax_df[hoax_df['Title'].isna()])

#Menghilangkan kata [SALAH] di judul Hoax
hoax_df = hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)]
print(hoax_df.head())

print("\nSetelah Remove [SALAH]")
# Menghapus kata '[SALAH]' dari kolom 'Title'
hoax_df['Title'] = hoax_df['Title'].str.replace(r'\[SALAH\]', '',regex=True)
print(hoax_df.head())


#Menghapus Tanda Kutip dari Judul Berita Hoax
# Menghilangkan tanda kutip di awal dan akhir teks dalam kolom 'Title'
print('\nSetelah Remove "" Tanda Kutip')
#hoax_df['Title'] = hoax_df['Title'].str.strip('“”')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('“”')
#hoax_df['Title'] = hoax_df['Title'].str.strip('"')
hoax_df['Title'] = hoax_df['Title'].str.strip().str.strip('""')

print(hoax_df)

print(hoax_df[hoax_df['Title'].str.contains(r'\[SALAH\]', regex=True)])

"""## Kombinasi Fact and Hoax"""

news_df = pd.concat([fact_df.iloc[:], hoax_df.iloc[:]], axis=0, ignore_index=True)
#news_df = pd.concat([fact_df.iloc[:], fact_df1.iloc[:],hoax_df.iloc[:],], axis=0, ignore_index=True)

print(news_df)

"""## Representasi Jumlah Data"""

# Menghitung jumlah angka 0 dan 1 dalam kolom "hoax"
hoax_count = news_df['hoax'].value_counts()

# Warna untuk masing-masing nilai
colors = ['blue', 'orange']

# Membuat diagram batang dengan warna yang berbeda untuk masing-masing nilai
plt.bar(hoax_count.index, hoax_count.values, color=colors)
#plt.xlabel('Hoax')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Tidak Hoax', 'Hoax'])
plt.title('Distribusi Data Berita per Kategori')
plt.show()

# download corpus punkt
nltk.download('punkt')

text = "Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan"
print(word_tokenize(text))

print(news_df.dtypes)

news_df.count()

"""# Preprocessing

## FUNGSI PREPROCESSING FIX
"""

# Contoh penggunaan:
news_df = preprocessing(news_df)

print(news_df)

"""## WordCloud Real , RUN SETELAH REMOVE STOPWORD"""

# Contoh penggunaan fungsi untuk non-hoax
generate_and_display_wordcloud(news_df, label_value=0)

# Contoh penggunaan fungsi untuk hoax
generate_and_display_wordcloud(news_df, label_value=1)

"""## Split Train Test"""

# grader-required-cell

#NUM_WORDS = 12000
EMBEDDING_DIM = 50
PADDING = 'post'
OOV_TOKEN = "<OOV>"
TRAINING_SPLIT = .8

# Splitting the data into 80% training and 20% testing
#x_train, x_test, y_train, y_test = train_test_split(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)
# Gantilah news_df['Title'] dan news_df['hoax'] sesuai dengan nama kolom yang sesuai dalam DataFrame Anda.
x_train, x_test, y_train, y_test = split_data(news_df['Title'], news_df['hoax'], test_size=0.2, random_state=42)

print("Jumlah data dalam set pelatihan:", x_train.shape[0])
print("Jumlah data dalam set pengujian:", x_test.shape[0])

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(x_train)

print(x_test)

print(y_train)

print(y_test)

# Membuat DataFrame
df_result = pd.DataFrame({
    'Text': x_test,  # Gantilah dengan nama kolom sesuai dengan data yang ada pada X_test
    'True_Label': y_test,  # Gantilah dengan nama kolom sesuai dengan data yang ada pada y_test
})

# Reset index
df_result = df_result.reset_index(drop=True)

print(df_result)

"""## Tokenizing dan Padding"""

"""## 4.Tokenizing"""

#Tokenizing Text -> Repsesenting each word by a number
#tokenizer = Tokenizer(num_words = NUM_WORDS,oov_token = OOV_TOKEN)
max_features = 15000
maxlen = 30

tokenizer = Tokenizer(num_words=max_features,oov_token = OOV_TOKEN)
tokenizer.fit_on_texts(x_train)
tokenized_train = tokenizer.texts_to_sequences(x_train)

#padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)
x_train = pad_sequences(tokenized_train,padding=PADDING, maxlen=maxlen)

tokenized_test = tokenizer.texts_to_sequences(x_test)
X_test = pad_sequences(tokenized_test,padding=PADDING ,maxlen=maxlen)

print(x_train)

print(x_train[24])

word_index = tokenizer.word_index

#print(word_index)

print(len(word_index))

"""## Model Word2Vec 50 Dimension"""

import numpy as np
from nltk.tokenize import word_tokenize

embeddings_dictionary = {}
with open('C:\\Users\\user\\PycharmProjects\\Deep_Learning\\Klasifikasi Berita Hoax Indonesia Word2Vec\\Indo Word2Vec\\word2vec.txt') as fp:
    for line in fp.readlines():
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions

vocab_length = 15000 # modify
embedding_dim = 50

embedding_matrix = np.zeros((vocab_length, embedding_dim))

for word, index in tokenizer.word_index.items(): # change depending on your data
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

vocab_length = len(embeddings_dictionary)
print("Vocabulary Length:", vocab_length)

print(embedding_matrix)

print(embedding_vector)

# Get the dimensions from the loaded embeddings
print(embedding_vector.shape[0])

print(embedding_matrix.shape[0])

"""### Cek vektor kata dan yang berdekatan"""

from sklearn.metrics.pairwise import cosine_similarity

# Mencari vektor kata "saya"
vector_saya = embeddings_dictionary.get("saya")

# Mencari kata-kata berdekatan berdasarkan cosine similarity
similar_words = {}
for word, vector in embeddings_dictionary.items():
    similarity = cosine_similarity([vector_saya], [vector])[0][0]
    similar_words[word] = similarity

# Mengurutkan kata-kata berdasarkan similarity (dalam urutan menurun)
similar_words = sorted(similar_words.items(), key=lambda x: x[1], reverse=True)

# Menampilkan hasil
print("Kata-kata berdekatan dengan 'saya':")
for word, similarity in similar_words[:10]:  # Ambil 10 kata teratas
    print(f"{word}: {similarity}")

"""## Build Model with Word2Vec

### Model 1
"""

max_features = 15000
batch_size = 64
epochs = 6
# 10 terlalu banyak , overfitting
embed_size = 50

# Function to build the model
"""## MODEL

### Model 1

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.01
"""

def build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate):
    max_features =  15000# Isi dengan nilai sesuai kebutuhan
    maxlen =  30 # Isi dengan nilai sesuai kebutuhan
    # x_train, y_train, X_test, y_test harus sudah didefinisikan sebelumnya

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(
        input_dim=max_features,
        output_dim=embedding_dim,  # Adjusted to match the pre-trained embedding dimension
        weights=[embedding_matrix],
        input_length=maxlen,
        trainable=False  # Optional: Set to True if you want to fine-tune the embeddings
    ),
        #tf.keras.layers.Embedding(input_dim=max_features, output_dim=embed_size, input_length=maxlen),
        tf.keras.layers.LSTM(hidden_layer_size),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])

    model.summary()

    history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), epochs=epochs)

    return history,model

# Contoh pemanggilan fungsi:
#epochs = 5
#embed_size = 50
hidden_layer_size = 64
#batch_size = 64
learning_rate = 0.01

print("Model 1")
# Contoh pemanggilan untuk dua model dengan nama yang berbeda
history,model = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

# Usage
evaluate_and_visualize(model, x_train, y_train, X_test, y_test)

# Example usage
input_sentences = ["anies meninggal dunia", "jokowi minta maaf kepada pki","gede pasek doakan ahy menjadi capres","CAK NUN SEBUT JOKOWI SEPERTI FIR’AUN KARENA DISURUH",
                   "Beban Utang Prabowo Subianto 7,6 Triliun"]

predictions = predict_sentences(input_sentences, model, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 2"""

"""### Model 2

*   **Epoch** : 5
*   **Dimension Embedding** : 100
*   **Hidden Layer**  : 64
*   **Batch Size**    : 64
*   **Optimizer**     : Adam
*   **Learning Rate**  : 0.001

"""

#epochs = 5
#embed_size = 50
hidden_layer_size = 64
batch_size = 64
learning_rate = 1e-2

print("Model 2")
history2,model2 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history2, "accuracy")
plot_graphs(history2, "loss")

# Usage
evaluate_and_visualize(model2, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model2, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 3"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 64
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 3")
history3,model3 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history3, "accuracy")
plot_graphs(history3, "loss")

# Usage
evaluate_and_visualize(model3, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model3, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 4"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-1

print("Model 4")
history4,model4 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history4, "accuracy")
plot_graphs(history4, "loss")

# Usage
evaluate_and_visualize(model4, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model4, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 5"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-2

print("Model 5")
history5,model5 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history5, "accuracy")
plot_graphs(history5, "loss")

# Usage
evaluate_and_visualize(model5, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model5, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

"""### Model 6"""

#epochs = 5
#embed_size = 100
hidden_layer_size = 64
batch_size = 32
optimizer = 'Adam'
learning_rate = 1e-3

print("Model 6")
history6,model6 = build_and_train_model(epochs, embed_size, hidden_layer_size, batch_size, learning_rate)

plot_graphs(history6, "accuracy")
plot_graphs(history6, "loss")

# Usage
evaluate_and_visualize(model6, x_train, y_train, X_test, y_test)

# Example usage

predictions = predict_sentences(input_sentences, model6, tokenizer, maxlen)

for sentence, prediction in zip(input_sentences, predictions):
    print(f"Sentence: {sentence}\nPrediction: {'Hoax' if prediction[0] == 1 else 'Real'}\n")

## TEST KODE LABEL PREDICT == REAL
# Misalkan Anda sudah mempunyai DataFrame df_result
# df_result = ...

# Misalnya, sentences adalah kolom 'text' pada DataFrame
sentences = df_result['Text']

max_len =30
# Memanggil fungsi predict_sentences untuk memperoleh prediksi biner
binary_predictions = predict_sentences(sentences, model, tokenizer, max_len)

# Menambahkan kolom 'predicted_label' ke DataFrame
df_result['predicted_label'] = binary_predictions

# Menambahkan kolom 'is_correct' yang berisi label true jika 'predicted_label' sama dengan 'true_label'
df_result['is_correct'] = df_result['predicted_label'] == df_result['True_Label']

print(df_result)

import pandas as pd

# Misalkan Anda sudah mempunyai DataFrame df_result
# df_result = ...

# Menghitung jumlah label true dan false dari kolom 'is_correct'
label_counts = df_result['is_correct'].value_counts()

# Menampilkan hasil
print("Jumlah label True:", label_counts[True])
print("Jumlah label False:", label_counts[False])

## FALSE POSITIVE
# Misalkan Anda sudah mempunyai DataFrame df_result
# df_result = ...

# Menampilkan data di mana label asli adalah 0 tetapi diprediksi 1
false_positive = df_result[(df_result['True_Label'] == 0) & (df_result['predicted_label'] == 1)]

# Menampilkan data di mana label asli adalah 1 tetapi diprediksi 0
false_negative = df_result[(df_result['True_Label'] == 1) & (df_result['predicted_label'] == 0)]

# Menampilkan hasil
print("Data False Positive:")
print(false_positive)

## FALSE NEGATIVE
print("\nData False Negative:")
print(false_negative)

# Menyimpan DataFrame yang sudah diperbarui
df_result.to_csv('df_result_updated.csv', index=False)  # Gantilah dengan nama file dan path yang sesuai